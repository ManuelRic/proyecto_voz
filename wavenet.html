<!DOCTYPE html>
<html lang="es">
<head>
    <link rel="icon" href="img/ev_1/logo_pro.png">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>WaveNet - DeepMind</title>
    <style>
        body {
            background: linear-gradient(135deg, #121212 0%, #1a1a1a 100%);
            color: #E0E0E0;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 0;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }

        header {
            text-align: center;
            margin-bottom: 40px;
        }

        h1 {
            font-size: 48px;
            color: #c87100;
            margin-bottom: 10px;
        }

        .subtitle {
            font-size: 20px;
            color: #BBBBBB;
            font-weight: 400;
            margin-top: 0;
        }

        .section, .card {
            margin-bottom: 40px;
            transition: transform 0.3s ease;
        }

        .section:hover, .card:hover {
            transform: translateY(-5px);
        }

        h2 {
            font-size: 32px;
            color: #FFD369;
            border-bottom: 2px solid #c87100;
            padding-bottom: 10px;
            margin-bottom: 20px;
        }

        p, ul {
            font-size: 18px;
            line-height: 1.8;
            color: #FFFFFF;
            margin: 10px 0;
        }

        ul {
            padding-left: 20px;
        }

        li {
            margin-bottom: 10px;
        }

        .highlight {
            color: #FFD369;
            font-weight: bold;
        }

        footer {
            text-align: center;
            margin-top: 40px;
            font-size: 14px;
            color: #BBBBBB;
        }

        #back_button{
            cursor: pointer;
            position:fixed;
            top: 0;
            left: 0;
            filter: invert(100%);
            width: 80px;
            margin: 20px;
        }
        
        #back_button:hover{
            filter: invert(80%);
        }

        .tooltip {
            position: relative;
            display: inline-block;
            color: #FFD369;
            font-weight: bold;
            cursor: pointer;
        }

        .tooltip-text {
            visibility: hidden;
            background-color: #333;
            color: #fff;
            text-align: center;
            padding: 5px 10px;
            border-radius: 5px;
            position: absolute;
            z-index: 1;
            bottom: 125%;
            left: 50%;
            transform: translateX(-50%);
            opacity: 0;
            transition: opacity 0.3s;
            white-space: nowrap;
            max-width: 90vw;
            overflow-wrap: break-word;
            box-sizing: border-box;
        }

        .tooltip:hover .tooltip-text {
            visibility: visible;
            opacity: 1;
        }

        .card {
            background: #1e1e1e;
            border-radius: 12px;
            padding: 25px;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.2);
            border-left: 4px solid #c87100;
            position: relative;
            overflow: hidden;
        }

        .card::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: linear-gradient(135deg, rgba(200, 113, 0, 0.1) 0%, transparent 100%);
            z-index: 0;
        }

        .card-content {
            position: relative;
            z-index: 1;
        }

        .card h2 {
            border-bottom: none;
            padding-bottom: 0;
            margin-bottom: 15px;
            color: #FFD369;
            font-size: 28px;
        }

        .audio-comparison {
            display: flex;
            gap: 20px;
            margin-top: 20px;
            flex-wrap: wrap;
        }

        .audio-sample {
            flex: 1;
            min-width: 250px;
            background: #1e1e1e;
            padding: 15px;
            border-radius: 8px;
            border-left: 3px solid #c87100;
        }

        .audio-player {
            width: 100%;
            margin: 10px 0;
        }

        .audio-description {
            font-size: 14px;
            color: #BBBBBB;
            margin-top: 5px;
            font-style: italic;
        }

        audio::-webkit-media-controls-panel {
            background-color: #2a2a2a;
        }

        audio::-webkit-media-controls-play-button,
        audio::-webkit-media-controls-mute-button {
            filter: invert(80%);
        }
        .spectrogram-container {
            display: flex;
            align-items: start;
            gap: 30px;
            margin: 30px 0;
        }

        .spectrogram-item {
            flex: 1;
            min-width: 0;
            text-align: start;
        }

        .spectrogram-item img {
            max-height: 400px;
            width: auto;
            max-width: 100%;
            border: 1px solid #444;
            border-radius: 4px;
        }

        .spectrogram-item p {
            margin-top: 10px;
            color: #BBBBBB;
            font-size: 16px;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .spectrogram-container {
                flex-direction: column;
                align-items: start;
            }
            
            .spectrogram-item {
                width: 100%;
                margin-bottom: 20px;
            }
            
            .spectrogram-item img {
                max-height: 250px;
            }
        }

                .spectrogram-analysis {
            background: #1e1e1e;
            border-radius: 12px;
            padding: 25px;
        }

        .spectrogram-caption {
            background: rgba(40, 40, 40, 0.7);
            padding: 15px;
            border-radius: 8px;
            margin-top: 15px;
        }

        .spectrogram-caption h3 {
            color: #FFD369;
            font-size: 20px;
            margin-top: 0;
            border-bottom: 1px solid #c87100;
            padding-bottom: 8px;
        }

        .spectrogram-caption ul {
            padding-left: 20px;
        }

        .spectrogram-caption li {
            margin-bottom: 8px;
            font-size: 16px;
            line-height: 1.5;
            align-items: start;
        }

        .technical-conclusion {
            background: rgba(200, 113, 0, 0.1);
            border-left: 4px solid #c87100;
            padding: 20px;
            margin-top: 30px;
            border-radius: 8px;
        }

        .technical-conclusion h3 {
            color: #FFD369;
            margin-top: 0;
        }

        .technical-conclusion ol {
            padding-left: 25px;
        }

        .technical-conclusion li {
            margin-bottom: 10px;
        }

        @media (max-width: 768px) {
            .spectrogram-caption {
                padding: 10px;
            }
            
            .spectrogram-caption li {
                font-size: 14px;
            }
        }
    </style>
</head>
<body>

    <div class="container">
        <header>
            <h1>WaveNet</h1>
            <p class="subtitle">Modelo generativo autoregresivo para la síntesis de audio realista</p>
        </header>

        <img id="back_button" src="img/ev_1/home.png">

        <section class="card">
            <h2>¿Qué es?</h2>
            <div class="card-content">
                <p>
                    WaveNet es un modelo generativo autoregresivo desarrollado por DeepMind que produce directamente ondas de audio crudas (*raw waveforms*). A diferencia de los métodos tradicionales que utilizan concatenación o vocoders paramétricos, WaveNet genera audio muestra por muestra, modelando su distribución probabilística condicional de forma precisa, es decir, es un modelo basado en la síntesis neuronal.
                </p>
            </div>
        </section>

        <section class="section">
            <h2>Convoluciones Causales Dilatadas</h2>
            <p>
                Para preservar la direccionalidad temporal (pasado → futuro), WaveNet emplea <span class="highlight">convoluciones causales</span>. Estas aseguran que cada predicción solo dependa del presente y el pasado, nunca del futuro.
            </p>
            <p>
                Sin embargo, una sola capa de <span class="tooltip"><i>convolución</i><span class="tooltip-text">Operación que se aplica sobre datos (como imágenes, señales de audio o texto) para extraer patrones locales.</span></span> causal tiene un contexto limitado. Para remediar esto, se utilizan <span class="highlight">convoluciones dilatadas</span>, que expanden el campo receptivo de la red exponencialmente sin incrementar drásticamente los parámetros.
            </p>
            
            <img id="wavenet1" src="img/ev_1/wavenet_2.gif">
            <img id="wavenet2" src="img/ev_1/wavenet.gif">
            
            <p>Por otro lado tenemos la <span class="tooltip">dilatación<span class="tooltip-text"><img id="dilatación" style="width: 400px;"  src="img/ev_1/dilatacion.jpg"></span></span> que permite que la red tenga más contexto que con una convolución normal. Es similar a las convoluciones de agrupamiento o estriadas, pero la salida tiene el mismo tamaño que la entrada.</p>
            
        </section>

        <section class="section">
            <h2>Distribuciones Softmax</h2>
            <p>
                 La función softmax es una función matemática que convierte un vector de números reales en una distribución de probabilidad. 
                 Es ampliamente utilizada en redes neuronales, especialmente en las capas de salida cuando se quiere modelar una variable categórica, 
                 como en la generación de texto o audio donde hay que elegir el siguiente símbolo (por ejemplo, una muestra de audio cuantizada).
                 Para modelar la distribución de convoluciones causales dilatadas <span class="tooltip">(Distribución Condicional)</span>
                 usamos la función Softmax ya que puede manejar una gran cantidad de posibles valores de salida y es lo suficientemente flexible para 
                 capturar la complejidad de las señales de audio. 
            </p>
            <p>
                Antes de aplicar la función Softmax, los valores de audio se transforman utilizando la µ-law (ley A), dónde μ = 255. Después de aplicar µ-law,
                 los valores de audio transformados se cuantizan y en el caso de WaveNet, se cuantiza con 256 niveles.
            </p>
            <p>
                Después de cuantizar la señal de audio, cada valor cuantizado representa un nivel diferente. 
                La función Softmax convierte los logits (valores de entrada) en probabilidades para cada uno de los niveles además se asegura
                 que la suma de todas las probabilidades sea igual a 1, permitiendo interpretar los valores como probabilidades 
                de pertenencia a cada uno de los 256 niveles.
            </p>
            <div style="text-align: center;justify-content: center;">
            <img src="img/ev_1/wavenet_3.png">
            </div>
            <p>
                Si el nivel 80 tiene una probabilidad de 0.04, esto significa que, según la función Softmax, la probabilidad de que la siguiente muestra de audio sea cuantizada al nivel 80 es del 4%.
En otras palabras, de todas los posibles niveles cuantizados, el nivel 80 tiene una probabilidad del 4% de ser el valor de la siguiente muestra de audio.

            </p>
        </section>

    <section class="section">
        <h2>Context stacks</h2>
        <p>
            Los Context Stacks son una estructura complementaria que se añade encima del WaveNet principal para aumentar aún más su capacidad para capturar dependencias a largo plazo,
            con ello logramos recordar y usar información de eventos pasados lejanos en el tiempo, cuando está generando o procesando una nueva parte de la señal.
        </p>
        <ul>
            <li>Mantener coherencia en el tono.</li>
            <li>Recordar dónde están las pausas.</li>
            <li>Adaptar la entonación final como en una pregunta (en “¿cómo estás?”).</li>
            <li>Usar un ritmo y prosodia natural desde el principio hasta el final.</li>
        </ul>
        <p>
            Esto ayuda a que al haccer la narración del texto suene fluido sin y natural con pausas del habla imitando el compartamiento más humano.
            Aunque WaveNet ya usa convoluciones dilatadas para aumentar su campo de visión, añadir estas capas ayuda a no aumentar tanto el coste computacional.   
        </p>
    </section>

    <section class="section">
        <h2>Redes de Ondas Condicionales</h2>
        <p>
            WaveNet puede ser extendido para generar audio condicionado a diferentes tipos de entradas. Estas <span class="highlight">redes de ondas condicionales</span>
             permiten que el modelo produzca diferentes resultados basados en contexto adicional.
        </p>
        <ul>
            <li><b>Texto</b>: Condicionamiento con embeddings de texto para síntesis de voz (TTS).</li>
            <li><b>Altavoz</b>: Embeddings del hablante para personalizar la voz generada.</li>
            <li><b>Etiquetas musicales</b>: Para generar música específica por género o instrumento.</li>
        </ul>
    </section>
    <section class="section">
        <h2>Modelos complementarios</h2>
        <p>
            Tacotron es un modelo secuencia-a-secuencia (seq2seq) que convierte texto directamente en espectrogramas mel, 
            los cuales luego pueden ser transformados en audio usando WaveNet como vocoder. Este enfoque combinado es la base 
            de sistemas como Google Cloud TTS.
        </p>
        <ul>
            <li><strong>Arquitectura:</strong> Usa redes neuronales recurrentes (RNN) y atención para alinear texto con características acústicas.</li>
            <li><strong>Integración con WaveNet:</strong> Tacotron 2 genera espectrogramas mel que WaveNet convierte en ondas de audio realistas.</li>
        </ul>
    </section>

</div>
    
<section class="card">
    <h2>Demo Comparativa: WaveNet vs Síntesis Paramétrica</h2>
    
    <!-- Comparación de Audios -->
    <div class="audio-comparison">
        <div class="audio-sample">
            <h3>WaveNet (Síntesis Neuronal)</h3>
            <audio controls class="audio-player">
                <source src="audios/us-english-wavenet-1.wav" type="audio/wav">
                Tu navegador no soporta el elemento de audio.
            </audio>
            <p class="audio-description">Tecnología generativa basada en redes neuronales profundas</p>
        </div>
        
        <div class="audio-sample">
            <h3>Método Paramétrico</h3>
            <audio controls class="audio-player">
                <source src="audios/us-english-parametric-1.wav" type="audio/wav">
                Tu navegador no soporta el elemento de audio.
            </audio>
            <p class="audio-description">Síntesis tradicional basada en unidades acústicas</p>
        </div>
    </div>

    <!-- Explicación de diferencias auditivas -->
    <div class="audio-differences">
        <h3>Diferencias Audibles</h3>
        <ul>
            <li><strong>Naturalidad:</strong> WaveNet presenta variaciones micro-prosódicas imperceptibles en el método paramétrico</li>
            <li><strong>Transiciones:</strong> Los fonemas se conectan fluidamente en WaveNet versus cortes perceptibles en el paramétrico</li>
            <li><strong>Respiración:</strong> WaveNet modela sutiles sonidos de aspiración que dan realismo</li>
        </ul>
    </div>

    <!-- Análisis de Espectrogramas -->
    <div class="spectrogram-analysis">
        <h3>Análisis Visual de Espectrogramas</h3>
        <div class="spectrogram-container">
            <div class="spectrogram-item">
                <img src="img/ev_1/espctrogramama1.png" alt="Espectrograma WaveNet">
                <div class="spectrogram-caption">
                    <h4 style="text-align: center;">Características WaveNet</h4>
                    <ul>
                        <li><strong>Estructura armónica limpia:</strong> Bandas de frecuencia paralelas y continuas</li>
                        <li><strong>Transiciones suaves:</strong> Cambios graduales entre consonantes y vocales</li>
                        <li><strong>Detalle en altas frecuencias:</strong> Componentes de fricación bien definidos (/s/, /f/)</li>
                    </ul>
                </div>
            </div>
            
            <div class="spectrogram-item">
                <img src="img/ev_1/espctrogramama2.png" alt="Espectrograma Paramétrico">
                <div class="spectrogram-caption">
                    <h4 style="text-align: center;">Limitaciones Paramétricas</h4>
                    <ul>
                        <li><strong>Artefactos de concatenación:</strong> Líneas verticales marcadas entre unidades</li>
                        <li><strong>Pérdida de armónicos:</strong> Bandas de frecuencia interrumpidas</li>
                        <li><strong>Falta de detalle:</strong> Áreas borrosas en consonantes fricativas</li>
                    </ul>
                </div>
            </div>
        </div>
    </div>

    <!-- Conclusión Técnica -->
    <div class="technical-summary">
        <h3>Interpretación Científica</h3>
        <p>Los espectrogramas revelan cómo WaveNet supera las limitaciones del método paramétrico:</p>
        <ol>
            <li><strong>Modelado de onda completa:</strong> Generación muestral que preserva micro-detalles acústicos</li>
            <li><strong>Campos receptivos dilatados:</strong> Capacidad para mantener coherencia a largo plazo</li>
            <li><strong>Distribuciones condicionales:</strong> Softmax de 256 niveles captura nuances espectrales</li>
        </ol>
        <p>Esta superioridad técnica se traduce en una experiencia auditiva indistinguible de voz humana en aplicaciones reales.</p>
    </div>
</section>

</body>

<script>
        document.addEventListener('DOMContentLoaded', function() {
    // Pausar el otro audio cuando uno se reproduce
    const audioPlayers = document.querySelectorAll('.audio-player');
    
    audioPlayers.forEach(player => {
        player.addEventListener('play', function() {
            audioPlayers.forEach(otherPlayer => {
                if (otherPlayer !== player) {
                    otherPlayer.pause();
                }
            });
        });
    });
});
        document.addEventListener("DOMContentLoaded", function () {
        // Back button functionality to go to page_1.html
        const backButton = document.getElementById("back_button");

        backButton.addEventListener("click", function() {
            window.location.href = "page_1.html";  // Redirect to page_1.html
        });
    });
</script>
</html>
